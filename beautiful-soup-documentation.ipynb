{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c29ce67-3fbc-48e5-857f-0d4e9c0c6220",
   "metadata": {},
   "source": [
    "## Goals:\n",
    "\n",
    "- Read the BS4 documentation at https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "- Parse the documentation\n",
    "- Read it into an in-memory Chroma instance\n",
    "- See if we can query against it with \"How can I use BeautifulSoup to get elements by CSS selector?\"\n",
    "\n",
    "## Notes:\n",
    "- The documentation is contained in `<section>` tags directly within the body element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac1a3130-0d34-4b27-8ecc-9ff4c343fe38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --quiet bs4 chromadb requests langchain langchain_openai pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0e8a80-17d3-40c2-a26e-54b38cd757ef",
   "metadata": {},
   "source": [
    "# Retrieve the Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5341f46-38af-4e71-b2e1-dad808f83d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BS4_DOCUMENTATION_URL=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67db3bc3-a3dc-4cd2-bccc-b91cea825c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import requests\n",
    "from operator import itemgetter\n",
    "from chromadb import Client\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from pydantic import BaseModel\n",
    "from typing import Dict, List, Optional, Sequence\n",
    "from langchain.schema import Document\n",
    "from langchain.schema.retriever import BaseRetriever\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.language_model import BaseLanguageModel\n",
    "from langchain.schema.runnable import (Runnable, RunnableBranch,\n",
    "                                       RunnableLambda, RunnableMap)\n",
    "from langchain_community.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3779ee71-7b2d-4107-bad1-151430e5860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the raw page content\n",
    "response = requests.get(BS4_DOCUMENTATION_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2a3be45-5b67-4ad8-8b99-65a33c9610c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metadata from a section\n",
    "def extract_metadata(section):\n",
    "    results = {}\n",
    "\n",
    "    # Extract the section ID\n",
    "    results[\"section-id\"] = section.attrs[\"id\"]\n",
    "\n",
    "    # Extract the section title\n",
    "    section_title = section.find(\"h1\")\n",
    "    if section_title:\n",
    "        results[\"section-title\"] = section_title.get_text()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Split the section text into Documents\n",
    "def split_into_documents(texts, metadatas):\n",
    "    splitter = text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=750,  # Set the desired chunk size (number of characters)\n",
    "        chunk_overlap=50,  # Set the desired overlap between chunks (number of characters)\n",
    "        length_function=len,  # Specify the function to measure the length of the text\n",
    "        is_separator_regex=False  # Specify whether the separators are regular expressions\n",
    "    )    \n",
    "    return splitter.create_documents(texts, metadatas)\n",
    "\n",
    "\n",
    "\n",
    "# Extract the documentation sections\n",
    "strainer = SoupStrainer(\"section\")\n",
    "soup = BeautifulSoup(response.content, \"html.parser\", parse_only=strainer)\n",
    "documentation_sections = soup.find_all(\"section\")\n",
    "\n",
    "\n",
    "sections = []\n",
    "metadatas = []\n",
    "\n",
    "for section in documentation_sections:\n",
    "    metadatas.append(extract_metadata(section))\n",
    "    sections.append(section.get_text())\n",
    "\n",
    "\n",
    "documents = split_into_documents(sections, metadatas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660d5ce3-1255-43c1-a5e7-dfc9c91e39c4",
   "metadata": {},
   "source": [
    "# Add the Documents to the VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9803d3e6-e16a-415d-8b15-eef8d8af41ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name=\"bs4_documentation\"\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "vector_store = Chroma(persist_directory=\"./chroma_db\", collection_name=collection_name,\n",
    "                   embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d970239-1be3-477f-bbbb-80364d154c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add_documents(documents)\n",
    "vector_store.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4600f50-d607-4a56-b18d-60dcd1d2237b",
   "metadata": {},
   "source": [
    "# Query the VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40557ea1-892d-454a-ab99-0270eb32fb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Document(page_content='.parents¶\\nYou can iterate over all of an element’s parents with\\n.parents. This example uses .parents to travel from an <a> tag\\nburied deep within the document, to the very top of the document:\\nlink = soup.a\\nlink\\n# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\\nfor parent in link.parents:\\n    print(parent.name)\\n# p\\n# body\\n# html\\n# [document]', metadata={'section-id': 'parents'}), 0.4240224063396454)\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I get an element's children?\"\n",
    "result = vector_store.similarity_search_with_score(question, k=10)\n",
    "result.sort(key=lambda input: input[1])\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32029e5a-9ff2-4952-886d-a40b5d04881e",
   "metadata": {},
   "source": [
    "# Query the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0722658f-829b-4f78-a144-84875dd8071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_TEMPLATE = \"\"\"\\\n",
    "You are an expert programmer and problem-solver, tasked with answering any question \\\n",
    "about Beautiful Soup 4.\n",
    "\n",
    "Generate a comprehensive and informative answer of 80 words or less for the \\\n",
    "given question based solely on the provided context. You must \\\n",
    "only use information from the provided context. Use an unbiased and \\\n",
    "journalistic tone. Combine search results together into a coherent answer. Do not \\\n",
    "repeat text.\n",
    "\n",
    "You should use bullet points in your answer for readability. Put citations where they apply\n",
    "rather than putting them all at the end.\n",
    "\n",
    "If there is nothing in the context relevant to the question at hand, just say \"Hmm, \\\n",
    "I'm not sure.\" Don't try to make up an answer.\n",
    "\n",
    "Anything between the following `context`  html blocks is retrieved from a knowledge \\\n",
    "bank, not part of the conversation with the user. \n",
    "\n",
    "<context>\n",
    "    {context} \n",
    "<context/>\n",
    "\n",
    "REMEMBER: If there is no relevant information within the context, just say \"Hmm, I'm \\\n",
    "not sure.\" Don't try to make up an answer. Anything between the preceding 'context' \\\n",
    "html blocks is retrieved from a knowledge bank, not part of the conversation with the \\\n",
    "user.\\\n",
    "\"\"\"\n",
    "\n",
    "REPHRASE_TEMPLATE = \"\"\"\\\n",
    "Given the following conversation and a follow up question, rephrase the follow up \\\n",
    "question to be a standalone question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone Question:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    question: str\n",
    "    chat_history: Optional[List[Dict[str, str]]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f184347e-db2a-428e-bef9-01cca3eaa7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags=['Chroma', 'OpenAIEmbeddings'] vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x11bd95c90>\n"
     ]
    }
   ],
   "source": [
    "def get_retriever() -> BaseRetriever:\n",
    "    return vector_db.as_retriever()\n",
    "\n",
    "print(get_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06cb9a7c-4098-4d2f-a242-253926290e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever_chain(\n",
    "    llm: BaseLanguageModel, retriever: BaseRetriever\n",
    ") -> Runnable:\n",
    "    CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(REPHRASE_TEMPLATE)\n",
    "    condense_question_chain = (\n",
    "        CONDENSE_QUESTION_PROMPT | llm | StrOutputParser()\n",
    "    ).with_config(\n",
    "        run_name=\"CondenseQuestion\",\n",
    "    )\n",
    "    conversation_chain = condense_question_chain | retriever\n",
    "    return RunnableBranch(\n",
    "        (\n",
    "            RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
    "                run_name=\"HasChatHistoryCheck\"\n",
    "            ),\n",
    "            conversation_chain.with_config(run_name=\"RetrievalChainWithHistory\"),\n",
    "        ),\n",
    "        (\n",
    "            RunnableLambda(itemgetter(\"question\")).with_config(\n",
    "                run_name=\"Itemgetter:question\"\n",
    "            )\n",
    "            | retriever\n",
    "        ).with_config(run_name=\"RetrievalChainWithNoHistory\"),\n",
    "    ).with_config(run_name=\"RouteDependingOnChatHistory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c61a4e4f-4da4-4060-a4cd-f94c8a5f4c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs: Sequence[Document]) -> str:\n",
    "    formatted_docs = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        doc_string = f\"<doc id='{i}'>{doc.page_content}</doc>\"\n",
    "        formatted_docs.append(doc_string)\n",
    "    return \"\\n\".join(formatted_docs)\n",
    "\n",
    "\n",
    "def serialize_history(request: ChatRequest):\n",
    "    chat_history = request[\"chat_history\"] or []\n",
    "    converted_chat_history = []\n",
    "    for message in chat_history:\n",
    "        if message.get(\"human\") is not None:\n",
    "            converted_chat_history.append(HumanMessage(content=message[\"human\"]))\n",
    "        if message.get(\"ai\") is not None:\n",
    "            converted_chat_history.append(AIMessage(content=message[\"ai\"]))\n",
    "    return converted_chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7568427-2ea6-413b-8066-5cf13225a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chain(\n",
    "    llm: BaseLanguageModel,\n",
    "    retriever: BaseRetriever,\n",
    ") -> Runnable:\n",
    "    retriever_chain = create_retriever_chain(\n",
    "        llm,\n",
    "        retriever,\n",
    "    ).with_config(run_name=\"FindDocs\")\n",
    "    _context = RunnableMap(\n",
    "        {\n",
    "            \"context\": retriever_chain | format_docs,\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "            \"chat_history\": itemgetter(\"chat_history\"),\n",
    "        }\n",
    "    ).with_config(run_name=\"RetrieveDocs\")\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", RESPONSE_TEMPLATE),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    response_synthesizer = (prompt | llm | StrOutputParser()).with_config(\n",
    "        run_name=\"GenerateResponse\",\n",
    "    )\n",
    "    return (\n",
    "        {\n",
    "            \"question\": RunnableLambda(itemgetter(\"question\")).with_config(\n",
    "                run_name=\"Itemgetter:question\"\n",
    "            ),\n",
    "            \"chat_history\": RunnableLambda(serialize_history).with_config(\n",
    "                run_name=\"SerializeHistory\"\n",
    "            ),\n",
    "        }\n",
    "        | _context\n",
    "        | response_synthesizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cadf2913-acda-4b08-9c66-249c66458bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To get an element's children using Beautiful Soup 4, you can use the `.children` attribute. Here's how you can do it:\n",
      "\n",
      "1. First, select the element you want to get the children of.\n",
      "2. Use the `.children` attribute on the selected element to retrieve its children.\n",
      "3. Iterate over the children using a loop or convert them to a list for further processing.\n",
      "\n",
      "Example:\n",
      "```python\n",
      "for child in element.children:\n",
      "    print(child)\n",
      "```\n",
      "\n",
      "This will print out each child element of the selected element.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo-16k\",\n",
    "    streaming=True,\n",
    "    temperature=0,\n",
    ")\n",
    "retriever = get_retriever()\n",
    "answer_chain = create_chain(\n",
    "    llm,\n",
    "    retriever,\n",
    ")\n",
    "\n",
    "inputs = {\"question\": question, \"chat_history\": None}\n",
    "\n",
    "result = answer_chain.invoke(inputs)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe38737-199a-47bb-81c0-df7be173df3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
